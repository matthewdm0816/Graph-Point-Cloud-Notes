\contentsline {section}{\numberline {1}NeVAE}{9}{section.1}%
\contentsline {subsection}{\numberline {1.1}Encoder}{9}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Decoder}{10}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Training}{11}{subsection.1.3}%
\contentsline {subsection}{\numberline {1.4}Property Oriented Mol. Gen.}{11}{subsection.1.4}%
\contentsline {section}{\numberline {2}Seminar on Self/Un-Supervised Learning @ 2020/9/16}{12}{section.2}%
\contentsline {subsection}{\numberline {2.1}Self-Learning @ Video Learning}{12}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Transformation Equivariance vs. Invariance @ Visial Repr. Learning}{14}{subsection.2.2}%
\contentsline {section}{\numberline {3}AET, AVT: Autoencoding Transformations}{16}{section.3}%
\contentsline {section}{\numberline {4}Flow-Based Generative Models}{18}{section.4}%
\contentsline {subsection}{\numberline {4.1}Outline \& Basics}{18}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}MoFlow}{19}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}GCF/Graph Conditional Flow}{21}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}Validity Correction \& Misc}{22}{subsubsection.4.2.2}%
\contentsline {subsection}{\numberline {4.3}GraphNVP}{23}{subsection.4.3}%
\contentsline {section}{\numberline {5}WGAN}{23}{section.5}%
\contentsline {section}{\numberline {6}GMMN+AE}{24}{section.6}%
\contentsline {subsection}{\numberline {6.1}Structure \& Idea}{24}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Training}{25}{subsection.6.2}%
\contentsline {section}{\numberline {7}FoldingNet - An AntoEncoder}{25}{section.7}%
\contentsline {section}{\numberline {8}PointFlow: Flow-based Generative Model on Point Clouds}{26}{section.8}%
\contentsline {subsection}{\numberline {8.1}Continuous Normalizing Flow(CNF)}{26}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Variational Auto-Encoder}{26}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}Model}{27}{subsection.8.3}%
\contentsline {section}{\numberline {9}FFJORD}{29}{section.9}%
\contentsline {subsection}{\numberline {9.1}CNF}{29}{subsection.9.1}%
\contentsline {subsection}{\numberline {9.2}Backpropagation through ODE Solutions with Adjoint Method}{29}{subsection.9.2}%
\contentsline {subsection}{\numberline {9.3}Unbiased Linear-Time Log-Density Estimation}{29}{subsection.9.3}%
\contentsline {section}{\numberline {10}Dequantization to Learn Discrete Distribution}{30}{section.10}%
\contentsline {subsection}{\numberline {10.1}Dequantization as Latent Variable Model}{30}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}Variational Dequantization}{31}{subsection.10.2}%
\contentsline {subsection}{\numberline {10.3}Importance-Weighted Dequantization}{31}{subsection.10.3}%
\contentsline {subsection}{\numberline {10.4}Renyi Dequantization}{32}{subsection.10.4}%
\contentsline {subsection}{\numberline {10.5}Dequantization Distribution}{32}{subsection.10.5}%
\contentsline {subsection}{\numberline {10.6}(Choice of) Continuous Distribution}{33}{subsection.10.6}%
\contentsline {section}{\numberline {11}DGI: Deep Graph Infomax}{33}{section.11}%
\contentsline {subsection}{\numberline {11.1}Backgrounds, Approach, Math}{33}{subsection.11.1}%
\contentsline {subsection}{\numberline {11.2}Algorithm}{34}{subsection.11.2}%
\contentsline {section}{\numberline {12}GraphSAGE: Inductive Representation Learning on Graph}{35}{section.12}%
\contentsline {subsection}{\numberline {12.1}Embedding Generation/FP}{35}{subsection.12.1}%
\contentsline {subsection}{\numberline {12.2}Aggragator Selection}{35}{subsection.12.2}%
\contentsline {section}{\numberline {13}SGC: Simplified Graph Convolution}{36}{section.13}%
\contentsline {section}{\numberline {14}FastGCN}{36}{section.14}%
\contentsline {subsection}{\numberline {14.1}Method}{36}{subsection.14.1}%
\contentsline {subsection}{\numberline {14.2}Variance Reduction}{37}{subsection.14.2}%
\contentsline {section}{\numberline {15}GWNN: Wavelet Transform on Graph}{38}{section.15}%
\contentsline {subsection}{\numberline {15.1}Supplementary Math: Real and Complex Wavelets}{38}{subsection.15.1}%
\contentsline {subsection}{\numberline {15.2}Graph Wavelets}{39}{subsection.15.2}%
\contentsline {subsection}{\numberline {15.3}GWNN}{40}{subsection.15.3}%
\contentsline {subsubsection}{\numberline {15.3.1}Details}{40}{subsubsection.15.3.1}%
\contentsline {section}{\numberline {16}Graph Wavelets}{40}{section.16}%
\contentsline {subsection}{\numberline {16.1}经典小波变换/CWT}{40}{subsection.16.1}%
\contentsline {subsection}{\numberline {16.2}谱小波变换/SGWT}{41}{subsection.16.2}%
\contentsline {subsubsection}{\numberline {16.2.1}Scaling Functions}{42}{subsubsection.16.2.1}%
\contentsline {subsection}{\numberline {16.3}SGWT的性质}{42}{subsection.16.3}%
\contentsline {subsubsection}{\numberline {16.3.1}Inverse SGWT}{42}{subsubsection.16.3.1}%
\contentsline {subsubsection}{\numberline {16.3.2}局域性}{42}{subsubsection.16.3.2}%
\contentsline {subsubsection}{\numberline {16.3.3}Spectral Wavelet Frames}{43}{subsubsection.16.3.3}%
\contentsline {subsection}{\numberline {16.4}Fast SGWT Approximation by Polynomials}{43}{subsection.16.4}%
\contentsline {subsubsection}{\numberline {16.4.1}Fast Approximation of Adjoint}{44}{subsubsection.16.4.1}%
\contentsline {subsubsection}{\numberline {16.4.2}Inverse Calculation}{45}{subsubsection.16.4.2}%
\contentsline {subsection}{\numberline {16.5}Implementations and Details}{45}{subsection.16.5}%
\contentsline {section}{\numberline {17}GMNN: Graph Markov Neural Network}{46}{section.17}%
\contentsline {subsection}{\numberline {17.1}Psedolikelihood Variaional EM}{46}{subsection.17.1}%
\contentsline {subsection}{\numberline {17.2}Inference}{47}{subsection.17.2}%
\contentsline {subsection}{\numberline {17.3}Learning}{48}{subsection.17.3}%
\contentsline {subsection}{\numberline {17.4}Optimization}{48}{subsection.17.4}%
\contentsline {section}{\numberline {18}ClusterGCN: Fast Deep \& Large GCNs}{49}{section.18}%
\contentsline {subsection}{\numberline {18.1}Vanilla ClusterGCN: Cluster For Batch}{49}{subsection.18.1}%
\contentsline {subsection}{\numberline {18.2}Stochastic Multiple Partitions}{50}{subsection.18.2}%
\contentsline {subsection}{\numberline {18.3}Analysis of Deeper Networks}{50}{subsection.18.3}%
\contentsline {section}{\numberline {19}GAT: Graph Attention Network}{51}{section.19}%
\contentsline {section}{\numberline {20}Note on Probalistic Graphical Models}{51}{section.20}%
\contentsline {subsection}{\numberline {20.1}Bayesian Networks}{51}{subsection.20.1}%
\contentsline {subsection}{\numberline {20.2}Undirected Networks}{52}{subsection.20.2}%
\contentsline {subsection}{\numberline {20.3}Local Probablistic Models | i.e. Specific Models Corresponds to Last 2 Sections}{54}{subsection.20.3}%
\contentsline {subsection}{\numberline {20.4}Temporal Models}{55}{subsection.20.4}%
\contentsline {section}{\numberline {21}RSCNN(CVPR 19')}{55}{section.21}%
\contentsline {subsection}{\numberline {21.1}Architecture}{55}{subsection.21.1}%
\contentsline {subsection}{\numberline {21.2}Details \& Implementation}{56}{subsection.21.2}%
\contentsline {section}{\numberline {22}SimpleView(ICLR 21' Candidate)}{56}{section.22}%
\contentsline {subsection}{\numberline {22.1}Simple Review of Existing Protocols}{56}{subsection.22.1}%
\contentsline {subsection}{\numberline {22.2}Model: SimpleView}{56}{subsection.22.2}%
\contentsline {section}{\numberline {23}OT-Flow}{57}{section.23}%
\contentsline {subsection}{\numberline {23.1}Idea \& Formulations}{57}{subsection.23.1}%
\contentsline {subsection}{\numberline {23.2}Parametrization of Model}{57}{subsection.23.2}%
\contentsline {subsection}{\numberline {23.3}Exact Hessian of Multilayer NN}{58}{subsection.23.3}%
\contentsline {section}{\numberline {24}Node2vec: Unsupervised Feature Learning}{58}{section.24}%
\contentsline {subsection}{\numberline {24.1}Basics}{58}{subsection.24.1}%
\contentsline {subsection}{\numberline {24.2}Biased Random Walk}{59}{subsection.24.2}%
\contentsline {subsection}{\numberline {24.3}Edge Feature}{60}{subsection.24.3}%
\contentsline {section}{\numberline {25}DeepWalk: Online Representation Learning}{60}{section.25}%
\contentsline {subsection}{\numberline {25.1}DeepWalk}{60}{subsection.25.1}%
\contentsline {subsection}{\numberline {25.2}SkipGram}{61}{subsection.25.2}%
\contentsline {subsection}{\numberline {25.3}Hierachichal Softmax}{61}{subsection.25.3}%
\contentsline {subsection}{\numberline {25.4}Parallelization}{62}{subsection.25.4}%
\contentsline {subsection}{\numberline {25.5}Variants}{62}{subsection.25.5}%
\contentsline {section}{\numberline {26}DAGNN: Towards Deeper GNN}{62}{section.26}%
\contentsline {subsection}{\numberline {26.1}Smoothness Metrics}{63}{subsection.26.1}%
\contentsline {subsection}{\numberline {26.2}Convergence of Propagation}{63}{subsection.26.2}%
\contentsline {subsection}{\numberline {26.3}DAGNN: Deep Adaptive GNN}{63}{subsection.26.3}%
\contentsline {section}{\numberline {27}t-SNE(t-Distributed Stochastic Neighbor Embedding)}{64}{section.27}%
\contentsline {subsection}{\numberline {27.1}SNE}{64}{subsection.27.1}%
\contentsline {subsection}{\numberline {27.2}UNI-SNE}{64}{subsection.27.2}%
\contentsline {subsection}{\numberline {27.3}t-SNE}{64}{subsection.27.3}%
\contentsline {subsection}{\numberline {27.4}Barnes-Hut-SNE}{65}{subsection.27.4}%
\contentsline {subsubsection}{\numberline {27.4.1}Approximating Input Simularities by Vantage-point Tree}{65}{subsubsection.27.4.1}%
\contentsline {subsubsection}{\numberline {27.4.2}Approximating t-SNE Gradients}{65}{subsubsection.27.4.2}%
\contentsline {section}{\numberline {28}Autoregressive Flows}{66}{section.28}%
\contentsline {subsection}{\numberline {28.1}Autoregressive Transformation}{66}{subsection.28.1}%
\contentsline {subsection}{\numberline {28.2}MAF: Masked Autoregressive Flow}{67}{subsection.28.2}%
\contentsline {subsection}{\numberline {28.3}IAF: Inverse Autoregressive Flow}{67}{subsection.28.3}%
\contentsline {subsection}{\numberline {28.4}Sylvester NF(UAI 18')}{67}{subsection.28.4}%
\contentsline {subsubsection}{\numberline {28.4.1}Idea}{67}{subsubsection.28.4.1}%
\contentsline {subsubsection}{\numberline {28.4.2}Parametrization of A \& B}{68}{subsubsection.28.4.2}%
\contentsline {subsubsection}{\numberline {28.4.3}Preserving Orthogonality of Q}{68}{subsubsection.28.4.3}%
\contentsline {section}{\numberline {29}Contrastive Multi-view Representation Learning/MVRLG(ICML 20')}{69}{section.29}%
\contentsline {subsection}{\numberline {29.1}Augmentations}{69}{subsection.29.1}%
\contentsline {subsection}{\numberline {29.2}Encoders}{69}{subsection.29.2}%
\contentsline {subsection}{\numberline {29.3}Training}{69}{subsection.29.3}%
\contentsline {section}{\numberline {30}GCC: Graph Contrastive Coding for GNN Pre-Training(KDD 20')}{71}{section.30}%
\contentsline {subsection}{\numberline {30.1}GCC Pre-Training}{71}{subsection.30.1}%
\contentsline {subsection}{\numberline {30.2}Finetuning GCC}{72}{subsection.30.2}%
\contentsline {section}{\numberline {31}GIN: Graph Isomorphism Network(ICLR 19')}{72}{section.31}%
\contentsline {subsection}{\numberline {31.1}Weisfeiler-Lehman Test}{72}{subsection.31.1}%
\contentsline {subsection}{\numberline {31.2}Math Intuitions}{73}{subsection.31.2}%
\contentsline {subsection}{\numberline {31.3}GIN}{73}{subsection.31.3}%
\contentsline {subsection}{\numberline {31.4}Graph Readout of GIN}{73}{subsection.31.4}%
\contentsline {section}{\numberline {32}GraphLoG: Self-Supervised Represtation Learning with Local \& Global Structure}{74}{section.32}%
\contentsline {subsection}{\numberline {32.1}Preliminaries}{74}{subsection.32.1}%
\contentsline {subsection}{\numberline {32.2}Local-Inst. Stru. Learning}{74}{subsection.32.2}%
\contentsline {subsection}{\numberline {32.3}Global-Semantic Repr. Learning}{75}{subsection.32.3}%
\contentsline {subsubsection}{\numberline {32.3.1}Init. of HP(Hirechichal Prototypes)}{75}{subsubsection.32.3.1}%
\contentsline {subsubsection}{\numberline {32.3.2}Maintainance of HP}{75}{subsubsection.32.3.2}%
\contentsline {subsection}{\numberline {32.4}Sup-GraphLoG: A Supervised Baseline}{76}{subsection.32.4}%
\contentsline {section}{\numberline {33}Orthogonal Weights in DNNs}{76}{section.33}%
\contentsline {subsection}{\numberline {33.1}Formulation \& Good Properties}{76}{subsection.33.1}%
\contentsline {subsection}{\numberline {33.2}OWN: Orthogonal Weight Normalization}{76}{subsection.33.2}%
\contentsline {subsubsection}{\numberline {33.2.1}Backpropagation}{77}{subsubsection.33.2.1}%
\contentsline {subsubsection}{\numberline {33.2.2}As Convolution}{77}{subsubsection.33.2.2}%
\contentsline {subsubsection}{\numberline {33.2.3}Group Based Orthogonalization: Divided Filters}{78}{subsubsection.33.2.3}%
\contentsline {section}{\numberline {34}OrthDNNs: Orthogonal DNNs(TPAMI 19')}{78}{section.34}%
\contentsline {subsection}{\numberline {34.1}GE Analysis in a Robustness and Isomeric Mapping Perspecitve}{78}{subsection.34.1}%
\contentsline {subsection}{\numberline {34.2}GE Analysis of DNN}{79}{subsection.34.2}%
\contentsline {subsection}{\numberline {34.3}OrthDNN by SVB(Singular Value Bound)}{79}{subsection.34.3}%
\contentsline {subsubsection}{\numberline {34.3.1}BN Compatibility}{80}{subsubsection.34.3.1}%
\contentsline {subsubsection}{\numberline {34.3.2}On CNN: OrthDNN as Convolution}{80}{subsubsection.34.3.2}%
\contentsline {section}{\numberline {35}SimCLR: A Simple Framework for Contrastive Learning of Visual Representations}{80}{section.35}%
\contentsline {subsection}{\numberline {35.1}Ideas \& Basics}{80}{subsection.35.1}%
\contentsline {section}{\numberline {36}BYOL: Build Your Own Latent}{81}{section.36}%
\contentsline {subsection}{\numberline {36.1}Ideas \& Method}{81}{subsection.36.1}%
\contentsline {section}{\numberline {37}MoCo: Momentum Contrast for Unsupervised Visual Representation Learning}{82}{section.37}%
\contentsline {subsection}{\numberline {37.1}Ideas \& Method}{82}{subsection.37.1}%
\contentsline {section}{\numberline {38}SimSiam: Exploring Simple Siamese Representation Learning}{83}{section.38}%
\contentsline {subsection}{\numberline {38.1}Intuitions}{83}{subsection.38.1}%
\contentsline {subsection}{\numberline {38.2}Method}{83}{subsection.38.2}%
\contentsline {section}{\numberline {39}Graph Layouts by t-SNE}{85}{section.39}%
\contentsline {subsection}{\numberline {39.1}Backgrounds}{85}{subsection.39.1}%
\contentsline {subsection}{\numberline {39.2}Method}{85}{subsection.39.2}%
\contentsline {subsection}{\numberline {39.3}tsNET}{85}{subsection.39.3}%
\contentsline {section}{\numberline {40}细粒度图像数据分类 by Xiangteng He}{85}{section.40}%
\contentsline {subsection}{\numberline {40.1}细粒度图像数据分类}{85}{subsection.40.1}%
\contentsline {subsection}{\numberline {40.2}RL-based 图像部件/对象识别}{85}{subsection.40.2}%
\contentsline {subsection}{\numberline {40.3}多层注意力区域辨识}{85}{subsection.40.3}%
\contentsline {subsection}{\numberline {40.4}多模态}{85}{subsection.40.4}%
\contentsline {section}{\numberline {41}Dirac Operator for Extrinstic Shape Analysis}{86}{section.41}%
\contentsline {subsection}{\numberline {41.1}Math}{86}{subsection.41.1}%
\contentsline {subsection}{\numberline {41.2}离散化}{87}{subsection.41.2}%
\contentsline {subsection}{\numberline {41.3}实值表示}{87}{subsection.41.3}%
\contentsline {subsection}{\numberline {41.4}有界区域: 边界条件}{87}{subsection.41.4}%
\contentsline {section}{\numberline {42}Mesh-Based Simulation with GNNs}{87}{section.42}%
\contentsline {subsection}{\numberline {42.1}结构}{87}{subsection.42.1}%
\contentsline {subsection}{\numberline {42.2}Adaptive Remeshing}{88}{subsection.42.2}%
\contentsline {section}{\numberline {43}SENet}{89}{section.43}%
\contentsline {section}{\numberline {44}Continuous-Time Spiking Neural Network}{90}{section.44}%
\contentsline {subsection}{\numberline {44.1}Neurons}{90}{subsection.44.1}%
\contentsline {subsection}{\numberline {44.2}Network Topology}{90}{subsection.44.2}%
\contentsline {subsection}{\numberline {44.3}突触塑性规则}{90}{subsection.44.3}%
\contentsline {section}{\numberline {45}Towards Deep Learning Models Resistant to Advsarial Attacks}{91}{section.45}%
\contentsline {subsection}{\numberline {45.1}Inner Maxmize Prob. :如何提出好的对抗样本}{91}{subsection.45.1}%
\contentsline {subsection}{\numberline {45.2}最小化问题}{91}{subsection.45.2}%
\contentsline {subsection}{\numberline {45.3}网络能力 \& 对抗健壮性}{92}{subsection.45.3}%
\contentsline {section}{\numberline {46}CAS: Channel-wise Activation Suppressing Module for Adversarial Robustness}{92}{section.46}%
\contentsline {section}{\numberline {47}Resisting Adversarial Attacks by $k$-Winners-Takes-All}{93}{section.47}%
\contentsline {subsection}{\numberline {47.1}Related Work: Obfuscated Gradients}{93}{subsection.47.1}%
\contentsline {subsection}{\numberline {47.2}$k$-Winners-Takes-All}{93}{subsection.47.2}%
\contentsline {subsection}{\numberline {47.3}Training}{94}{subsection.47.3}%
\contentsline {subsection}{\numberline {47.4}Theory Understand of the Discontinuity}{94}{subsection.47.4}%
\contentsline {subsection}{\numberline {47.5}Related Works}{94}{subsection.47.5}%
\contentsline {section}{\numberline {48}A Survey of Label-noise Representation Learning(LNRL): Past, Present and Future}{95}{section.48}%
\contentsline {subsection}{\numberline {48.1}Perspective of Data}{96}{subsection.48.1}%
\contentsline {subsection}{\numberline {48.2}Perspective of Statistics/Learning Theory}{96}{subsection.48.2}%
\contentsline {subsection}{\numberline {48.3}Perspective of Opt. Policy}{97}{subsection.48.3}%
\contentsline {subsection}{\numberline {48.4}Taxonomy}{97}{subsection.48.4}%
\contentsline {subsection}{\numberline {48.5}Data: Noise Trans. Mat.}{98}{subsection.48.5}%
\contentsline {subsection}{\numberline {48.6}Data: Adapt. Layer}{98}{subsection.48.6}%
\contentsline {subsection}{\numberline {48.7}Loss Correction: Backward/Forward Correction}{99}{subsection.48.7}%
\contentsline {subsection}{\numberline {48.8}Loss Correction: Gold Correction}{99}{subsection.48.8}%
\contentsline {subsection}{\numberline {48.9}Loss Correction: Label Smoothing}{100}{subsection.48.9}%
\contentsline {subsection}{\numberline {48.10}Prior Knowledge: Human-in-the-Loop Estimation}{100}{subsection.48.10}%
\contentsline {subsection}{\numberline {48.11}Prior Knowledge: Fine-tuning Revision}{100}{subsection.48.11}%
\contentsline {subsection}{\numberline {48.12}Regularization: Explicit Regularization}{101}{subsection.48.12}%
\contentsline {subsection}{\numberline {48.13}Objdective Regularization: Implicit}{102}{subsection.48.13}%
\contentsline {subsection}{\numberline {48.14}Objective Reweighting: Importnace Reweighting}{102}{subsection.48.14}%
\contentsline {subsection}{\numberline {48.15}Objective Reweighting: Bayesian Methods}{103}{subsection.48.15}%
\contentsline {subsection}{\numberline {48.16}Objective Reweighting: NNs}{103}{subsection.48.16}%
\contentsline {subsection}{\numberline {48.17}Objective Redesigning}{104}{subsection.48.17}%
\contentsline {subsection}{\numberline {48.18}Objective Redesigning: Loss Redesign}{104}{subsection.48.18}%
\contentsline {subsection}{\numberline {48.19}Label Ensemble}{105}{subsection.48.19}%
\contentsline {subsection}{\numberline {48.20}Optimization Policies}{106}{subsection.48.20}%
\contentsline {subsubsection}{\numberline {48.20.1}Memorizaiton Eff.}{106}{subsubsection.48.20.1}%
\contentsline {subsubsection}{\numberline {48.20.2}Self-training}{106}{subsubsection.48.20.2}%
\contentsline {subsubsection}{\numberline {48.20.3}Co-training}{107}{subsubsection.48.20.3}%
\contentsline {subsubsection}{\numberline {48.20.4}Beyond Memorizaiton}{108}{subsubsection.48.20.4}%
\contentsline {subsection}{\numberline {48.21}Future}{108}{subsection.48.21}%
\contentsline {subsubsection}{\numberline {48.21.1}Build Up New Datasets}{108}{subsubsection.48.21.1}%
\contentsline {subsubsection}{\numberline {48.21.2}Instance-Dependent LNRL}{108}{subsubsection.48.21.2}%
\contentsline {subsubsection}{\numberline {48.21.3}Adversarial LNRL}{109}{subsubsection.48.21.3}%
